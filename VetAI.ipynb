{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNdJhRQS4UzjgoThhLsXys",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdullahmujahidali/Vet-LangGraph/blob/main/VetAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulmxcc6C0j98",
        "outputId": "504469eb-3755-431f-d4d1-653ab732b6bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/412.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.7/412.7 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain langgraph langchain_openai openai python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Dict, Tuple, Annotated, TypedDict, Union, Any\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "import json\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0\n",
        ")"
      ],
      "metadata": {
        "id": "cUhG-GtS0mb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict):\n",
        "    messages: List[Union[HumanMessage, AIMessage]]\n",
        "    current_input: str\n",
        "    results: Dict[str, Any]\n",
        "\n",
        "\n",
        "def create_agent(name: str):\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are {name}, an AI agent responsible for processing user input. \"\n",
        "                  \"Analyze the input and provide your assessment.\"),\n",
        "        (\"human\", \"{input}\")\n",
        "    ])\n",
        "\n",
        "    def agent_function(state: AgentState):\n",
        "        current_input = state[\"current_input\"]\n",
        "\n",
        "        messages = prompt.format_messages(\n",
        "            name=name,\n",
        "            input=current_input\n",
        "        )\n",
        "\n",
        "        response = llm.invoke(messages)\n",
        "\n",
        "        new_state = state.copy()\n",
        "        new_state[\"messages\"].append(response)\n",
        "        new_state[\"results\"][name] = response.content\n",
        "\n",
        "        return new_state\n",
        "\n",
        "    return agent_function\n",
        "\n",
        "first_agent = create_agent(\"DataAnalyzer\")\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"analyze\", first_agent)\n",
        "\n",
        "workflow.set_entry_point(\"analyze\")\n",
        "\n",
        "workflow.add_edge(\"analyze\", END)\n",
        "\n",
        "app = workflow.compile()"
      ],
      "metadata": {
        "id": "Fu7LS24j1OZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_state = {\n",
        "    \"messages\": [],\n",
        "    \"current_input\": \"This is a test input that needs to be analyzed.\",\n",
        "    \"results\": {}\n",
        "}\n",
        "\n",
        "result = app.invoke(initial_state)\n",
        "\n",
        "print(\"Results:\", result[\"results\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_r23vdE1osE",
        "outputId": "d8dceb97-a987-4ca2-f302-144c8cd8b589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results: {'DataAnalyzer': 'Thank you for providing the input. As an AI DataAnalyzer, I can see that the input is a simple text string indicating that it is a test input that needs to be analyzed. It does not contain any specific data or information for analysis. If you have any specific data or questions you would like me to analyze, please provide them for further assessment.'}\n"
          ]
        }
      ]
    }
  ]
}